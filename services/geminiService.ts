import { GoogleGenAI, Type } from "@google/genai";
import { GenerateRequest, QAPair } from '../types';

// Initialize the API client
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

import SYSTEM_INSTRUCTION from '../prompts/v1.1_trend_optimization.txt?raw';

// ğŸš€ Optimized Model Strategy for Pro Users
// ğŸš€ Optimized Model Strategy for Cost Efficiency
const MODELS_TO_TRY = [
  'gemini-1.5-flash',       // âš¡ Fastest & Cheapest (Priority for Cost Saving)
  'gemini-1.5-pro',         // ğŸ¥‡ High Quality Fallback
  'gemini-2.0-flash',       // ğŸš€ Next Gen
  'gemini-1.0-pro',         // Legacy Fallback
];

// Helper to retry API calls with Model Fallback
const generateWithFallback = async (
  generateFn: (model: string) => Promise<any>
): Promise<string> => {
  let lastError: any = null;

  for (const model of MODELS_TO_TRY) {
    console.log(`[Google AI Pro] Attempting generation with model: ${model}...`);
    try {
      // 1. Try the model with robust retries for Pro tier
      const result = await callGeminiWithRetry(async () => {
        return await generateFn(model);
      }, 2, 1000); // 2 retries per model to ensure stability

      console.log(`âœ… SUCCESS: Model ${model} generated content.`);
      return result;

    } catch (error: any) {
      console.warn(`âš ï¸ Model ${model} failed, switching to next... Error:`, error.message || error);
      lastError = error;

      // If 403/Forbidden (API Key issue) -> Stop immediately
      if (error.response?.status === 403) throw error;
    }
  }

  // If all models fail
  throw new Error(`All models failed. Last error: ${lastError?.message || 'Unknown error'}`);
};

// Helper to retry API calls on 503/429 (Transient server errors)
const callGeminiWithRetry = async <T>(operation: () => Promise<T>, retries = 3, delay = 1000): Promise<T> => {
  try {
    return await operation();
  } catch (error: any) {
    const isOverloaded =
      error.status === 503 ||
      error.status === 429 || // Also retry on Rate Limit (Pro should have higher limits but still possible)
      (error.message && error.message.includes('Overloaded')) ||
      (error.message && error.message.includes('busy'));

    if (isOverloaded && retries > 0) {
      console.log(`â³ API Busy/Rate Limit. Retrying in ${delay}ms... (Retries left: ${retries})`);
      await new Promise(resolve => setTimeout(resolve, delay));
      return callGeminiWithRetry(operation, retries - 1, delay * 1.5); // Exponential backoff
    }
    throw error;
  }
};

export const generateExtendedQA = async (request: GenerateRequest): Promise<QAPair[]> => {
  const { inputArticle, ragContext } = request;

  // Prepare context string
  const contextString = ragContext.map((ctx, index) =>
    `[æ–‡ç«  ${index + 1}] ID: ${ctx.id}\næ¨™é¡Œ: ${ctx.title}\nå…§å®¹æ‘˜è¦: ${ctx.content}\n`
  ).join('\n----------------\n');

  const prompt = `
**ç›®æ¨™æ–‡ç« å…§å®¹**ï¼š
${inputArticle}

**æª¢ç´¢åˆ°çš„æ­·å²æ–‡ç«  (RAG Context)**ï¼š
${contextString}

**è«‹æ’°å¯« 6 çµ„å»¶ä¼¸å•ç­”ï¼Œä¸¦åš´æ ¼éµå®ˆä»¥ä¸‹åˆ†é…**ï¼š
1. **å„ªå…ˆæ’°å¯«è‡³å°‘ 3 çµ„** èˆ‡ã€Œæ­·å²æ–‡ç«  (RAG Context)ã€é«˜åº¦ç›¸é—œçš„å•ç­”ã€‚
   - é€™äº›å•ç­”çš„ Source å¿…é ˆæ˜¯ RAG æ–‡ç« çš„æ¨™é¡Œã€‚
   - å…§å®¹å¿…é ˆåŸºæ–¼æ­·å²æ–‡ç« çš„äº‹å¯¦ï¼Œ**åš´ç¦å¹»è¦º**ã€‚
2. **å‰©é¤˜çš„** å¯ä»¥æ˜¯åŸºæ–¼ã€Œç›®æ¨™æ–‡ç« ã€çš„å»¶ä¼¸ ([æœ¬æ–‡å»¶ä¼¸])ã€‚
3. å¦‚æœ RAG æ–‡ç« éå¸¸ç›¸é—œï¼Œæ‚¨å¯ä»¥ç”Ÿæˆè¶…é 3 çµ„ RAG å•ç­” (ä¾‹å¦‚ 4 çµ„ RAG + 2 çµ„ æœ¬æ–‡å»¶ä¼¸)ã€‚
4. çµ•å°ä¸å¯ä»¥å…¨éƒ¨éƒ½æ˜¯ [æœ¬æ–‡å»¶ä¼¸]ã€‚

è«‹ä»¥ JSON é™£åˆ—æ ¼å¼è¼¸å‡ºã€‚
`;

  try {
    const text = await generateWithFallback(async (model) => {
      const response = await ai.models.generateContent({
        model: model,
        contents: prompt,
        config: {
          systemInstruction: SYSTEM_INSTRUCTION,
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.ARRAY,
            items: {
              type: Type.OBJECT,
              properties: {
                question: { type: Type.STRING, description: "å•é¡Œæ¨™é¡Œï¼Œ15å­—ä»¥å…§ï¼Œå¿…é ˆæ˜¯å•å¥å½¢å¼ï¼ˆä¾‹å¦‚ï¼š...æ€éº¼æ­ï¼Ÿ...æ˜¯ä»€éº¼ï¼Ÿï¼‰" },
                answer: { type: Type.STRING, description: "å›ç­”å…§å®¹ï¼Œç´„800-1000å­—ï¼Œéœ€åŒ…å«3å€‹å°æ¨™é¡Œã€‚âš ï¸é‡è¦ï¼šå°æ¨™é¡Œ(**æ–‡å­—**)å¿…é ˆç¨ç«‹ä¸€è¡Œï¼Œå‰å¾Œè«‹æ›è¡Œã€‚åš´æ ¼ç¦æ­¢å‡ºç¾ [ID:xxxxx]ã€‚" },
                sourceId: { type: Type.STRING, description: "åƒè€ƒä¾†æºIDï¼Œä¾‹å¦‚ '57759' æˆ– 'æœ¬æ–‡å»¶ä¼¸'" },
                sourceTitle: { type: Type.STRING, description: "å¿…é ˆå®Œå…¨è¤‡è£½ RAG è³‡æ–™åº«ä¸­çš„ã€åŸå§‹å®Œæ•´æ¨™é¡Œã€‘ï¼Œç¦æ­¢ç°¡åŒ–æˆ–æ”¹å¯«ã€‚" },
              },
              required: ["question", "answer", "sourceId", "sourceTitle"],
            },
          },
        },
      });
      if (!response.text) throw new Error("No response from AI");
      return response.text;
    });

    // Parse output
    const data = JSON.parse(text);
    return data as QAPair[];

  } catch (error) {
    console.error("Gemini API All Models Failed:", error);
    throw error;
  }
};

/**
 * Generates a SINGLE QA pair based on the context.
 * Used for the "Redo" functionality.
 */
export const generateSingleQA = async (request: GenerateRequest): Promise<QAPair> => {
  const { inputArticle, ragContext } = request;

  const contextString = ragContext.map((ctx, index) =>
    `[æ–‡ç«  ${index + 1}] ID: ${ctx.id}\næ¨™é¡Œ: ${ctx.title}\nå…§å®¹æ‘˜è¦: ${ctx.content}\n`
  ).join('\n----------------\n');

  // Slightly modified prompt to ask for just one high-quality pair
  const prompt = `
**ç›®æ¨™æ–‡ç« å…§å®¹**ï¼š
${inputArticle}

**æª¢ç´¢åˆ°çš„æ­·å²æ–‡ç«  (RAG Context)**ï¼š
${contextString}

è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™ï¼Œæ’°å¯« **1 çµ„** å…¨æ–°çš„å»¶ä¼¸å•ç­” (Q&A)ï¼Œè«‹å˜—è©¦åˆ‡å…¥ä¸åŒçš„è§€é»ã€‚
`;

  try {
    const text = await generateWithFallback(async (model) => {
      const response = await ai.models.generateContent({
        model: model,
        contents: prompt,
        config: {
          systemInstruction: SYSTEM_INSTRUCTION, // Reuse the same persona
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.OBJECT, // Requesting a single Object, not Array
            properties: {
              question: { type: Type.STRING, description: "å•é¡Œæ¨™é¡Œï¼Œ15å­—ä»¥å…§ï¼Œå¿…é ˆæ˜¯å•å¥å½¢å¼ï¼ˆä¾‹å¦‚ï¼š...æ€éº¼æ­ï¼Ÿ...æ˜¯ä»€éº¼ï¼Ÿï¼‰" },
              answer: { type: Type.STRING, description: "å›ç­”å…§å®¹ï¼Œç´„800-1000å­—ï¼Œéœ€åŒ…å«3å€‹å°æ¨™é¡Œã€‚âš ï¸é‡è¦ï¼šå°æ¨™é¡Œ(**æ–‡å­—**)å¿…é ˆç¨ç«‹ä¸€è¡Œï¼Œå‰å¾Œè«‹æ›è¡Œã€‚åš´æ ¼ç¦æ­¢å‡ºç¾ [ID:xxxxx]ã€‚" },
              sourceId: { type: Type.STRING, description: "åƒè€ƒä¾†æºIDï¼Œä¾‹å¦‚ '57759' æˆ– 'æœ¬æ–‡å»¶ä¼¸'" },
              sourceTitle: { type: Type.STRING, description: "å¿…é ˆå®Œå…¨è¤‡è£½ RAG è³‡æ–™åº«ä¸­çš„ã€åŸå§‹å®Œæ•´æ¨™é¡Œã€‘ï¼Œç¦æ­¢ç°¡åŒ–æˆ–æ”¹å¯«ã€‚" },
            },
            required: ["question", "answer", "sourceId", "sourceTitle"],
          },
        },
      });
      if (!response.text) throw new Error("No response from AI");
      return response.text;
    });

    return JSON.parse(text) as QAPair;

  } catch (error) {
    console.error("Gemini API Single Generation Error:", error);
    throw error;
  }
};