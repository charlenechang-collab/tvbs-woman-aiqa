import { GoogleGenAI, Type } from "@google/genai";
import { GenerateRequest, QAPair } from '../types';
import SYSTEM_INSTRUCTION_TEMPLATE from '../prompts/v2_base_prompt.txt?raw';

// Initialize the API client
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

// ğŸš€ Local Cache Key for Daily Trends
const CACHE_KEY = 'daily_trends';

interface DailyTrends {
  date: string;
  keywords: string[];
}

/**
 * Get daily trends from localStorage or fetch new ones if cache is expired.
 * Implements client-side caching to reduce token usage and API calls.
 */
const getDailyTrends = async (): Promise<string> => {
  const today = new Date().toISOString().split('T')[0]; // YYYY-MM-DD
  const cachedData = localStorage.getItem(CACHE_KEY);

  if (cachedData) {
    try {
      const parsedCache: DailyTrends = JSON.parse(cachedData);
      // Check if cache is from today
      if (parsedCache.date === today && parsedCache.keywords && parsedCache.keywords.length > 0) {
        console.log('âœ… Cache Hit: Using stored daily trends.');
        return parsedCache.keywords.join('ã€');
      }
    } catch (e) {
      console.warn('Error parsing cached trends, fetching new ones.');
    }
  }

  console.log('âš ï¸ Cache Miss: Fetching new daily trends...');

  // In a real environment, this logic would trigger a "Web Search Skill" or call a backend API.
  // Since we are running client-side, we simulate the "Agent Search" result here.
  const keywords = await fetchNewTrends();

  // Summarize to top 5 keywords to save tokens
  const top5Keywords = keywords.slice(0, 5);

  // Save to cache
  const newCache: DailyTrends = {
    date: today,
    keywords: top5Keywords,
  };
  localStorage.setItem(CACHE_KEY, JSON.stringify(newCache));

  return top5Keywords.join('ã€');
};

/**
 * Fetches real-time trends using Gemini with Google Search Grounding.
 * This replaces the simulated "Agent Skill" with actual AI Web Search.
 */
const fetchNewTrends = async (): Promise<string[]> => {
  console.log('ğŸŒ Conducting Live Web Search for Trends...');

  // Calculate current year and month dynamically (e.g., "2026å¹´2æœˆ")
  const date = new Date();
  const currentYear = date.getFullYear();
  const currentMonth = date.getMonth() + 1; // getMonth is 0-indexed
  const dateString = `${currentYear}å¹´${currentMonth}æœˆ`;

  const searchPrompt = `
    è«‹æœå°‹ç›®å‰ ${dateString} å°ç£æœ€æµè¡Œçš„ã€Œç¾å¦ã€èˆ‡ã€Œæ™‚å°šã€é—œéµå­—ã€‚
    è«‹æ­¸ç´å‡ºæœ€ç†±é–€çš„ å‰ 5 å€‹ é—œéµå­— (ä¾‹å¦‚ï¼šç‰¹è‰²å¦å®¹ã€ç†±é–€æˆåˆ†ã€æµè¡Œè‰²ç³»)ã€‚
    
    å›å‚³æ ¼å¼è¦æ±‚ï¼š
    1. åªå›å‚³é—œéµå­—ï¼Œç”¨ã€Œã€ã€åˆ†éš”ã€‚
    2. ä¸è¦ markdownï¼Œä¸è¦å‰è¨€å¾Œèªã€‚
    3. æ¯å€‹é—œéµå­—å¯ä»¥é™„å¸¶è‹±æ–‡ (ä¾‹å¦‚ï¼šåŸç”Ÿæ„Ÿåº•å¦ (Native Skin))ã€‚
  `;

  try {
    // Call Gemini with Google Search Tool enabled
    const response = await ai.models.generateContent({
      model: 'gemini-1.5-flash', // Use Flash for speed & cost
      contents: searchPrompt,
      config: {
        tools: [{ googleSearch: {} }], // ğŸš€ Enable Live Search
        responseMimeType: "text/plain",
      }
    });

    const text = response.text;
    if (!text) throw new Error("Empty response from Trend Search");

    console.log('ğŸ” Raw Trend Search Result:', text);

    // Parse result (split by "ã€" or "," or newline)
    const keywords = text.split(/[,ã€\n]/)
      .map(k => k.trim())
      .filter(k => k.length > 0)
      .slice(0, 5); // Take top 5

    if (keywords.length === 0) throw new Error("Failed to parse keywords");

    return keywords;

  } catch (error) {
    console.error("âš ï¸ Trend Search Failed, using fallback list.", error);

    // Fallback list if Search fails (Backup Safety)
    return [
      'åŸç”Ÿæ„Ÿåº•å¦ (Native Skin)',
      'èœœç³–æ°´å…‰å”‡ (Honey Glazed Lips)',
      'æŸ”åŒ–å“¥å¾·é¢¨ (Soft Goth)',
      'ä¿®å®¹è…®ç´… (Contouring Blush)',
      'å¤–æ³Œé«”ä¿é¤Š (Exosomes)'
    ];
  }
};

// ğŸš€ Optimized Model Strategy: Forced Economy Mode
const MODELS_TO_TRY = [
  'gemini-1.5-flash',       // âš¡ Fastest & Cheapest (Forced as per v2.1 spec)
];

// Helper to retry API calls with Model Fallback
const generateWithFallback = async (
  generateFn: (model: string) => Promise<any>
): Promise<string> => {
  let lastError: any = null;

  for (const model of MODELS_TO_TRY) {
    console.log(`[Google AI] Attempting generation with model: ${model}...`);
    try {
      // 1. Try the model with robust retries
      const result = await callGeminiWithRetry(async () => {
        return await generateFn(model);
      }, 2, 1000); // 2 retries per model

      console.log(`âœ… SUCCESS: Model ${model} generated content.`);
      return result;

    } catch (error: any) {
      console.warn(`âš ï¸ Model ${model} failed... Error:`, error.message || error);
      lastError = error;

      // If 403/Forbidden (API Key issue) -> Stop immediately
      if (error.response?.status === 403) throw error;
    }
  }

  // If all models fail
  throw new Error(`All models failed. Last error: ${lastError?.message || 'Unknown error'}`);
};

// Helper to retry API calls on 503/429 (Transient server errors)
const callGeminiWithRetry = async <T>(operation: () => Promise<T>, retries = 3, delay = 1000): Promise<T> => {
  try {
    return await operation();
  } catch (error: any) {
    const isOverloaded =
      error.status === 503 ||
      error.status === 429 || // Also retry on Rate Limit
      (error.message && error.message.includes('Overloaded')) ||
      (error.message && error.message.includes('busy'));

    if (isOverloaded && retries > 0) {
      console.log(`â³ API Busy/Rate Limit. Retrying in ${delay}ms... (Retries left: ${retries})`);
      await new Promise(resolve => setTimeout(resolve, delay));
      return callGeminiWithRetry(operation, retries - 1, delay * 1.5); // Exponential backoff
    }
    throw error;
  }
};

export const generateExtendedQA = async (request: GenerateRequest): Promise<QAPair[]> => {
  const { inputArticle, ragContext } = request;

  // 1. Get Daily Trends (Cached or Fetched)
  const todayTrends = await getDailyTrends();

  // 2. Inject Trends into System Prompt (Dynamic Injection)
  const systemInstruction = SYSTEM_INSTRUCTION_TEMPLATE.replace('{{today_trends}}', todayTrends);

  // Prepare context string
  const contextString = ragContext.map((ctx, index) =>
    `[æ–‡ç«  ${index + 1}] ID: ${ctx.id}\næ¨™é¡Œ: ${ctx.title}\nå…§å®¹æ‘˜è¦: ${ctx.content}\n`
  ).join('\n----------------\n');

  const prompt = `
**ç›®æ¨™æ–‡ç« å…§å®¹**ï¼š
${inputArticle}

**æª¢ç´¢åˆ°çš„æ­·å²æ–‡ç«  (RAG Context)**ï¼š
${contextString}

**è«‹æ’°å¯« 6 çµ„å»¶ä¼¸å•ç­”ï¼Œä¸¦åš´æ ¼éµå®ˆä»¥ä¸‹åˆ†é…**ï¼š
1. **å„ªå…ˆæ’°å¯«è‡³å°‘ 3 çµ„** èˆ‡ã€Œæ­·å²æ–‡ç«  (RAG Context)ã€é«˜åº¦ç›¸é—œçš„å•ç­”ã€‚
   - é€™äº›å•ç­”çš„ Source å¿…é ˆæ˜¯ RAG æ–‡ç« çš„æ¨™é¡Œã€‚
   - å…§å®¹å¿…é ˆåŸºæ–¼æ­·å²æ–‡ç« çš„äº‹å¯¦ï¼Œ**åš´ç¦å¹»è¦º**ã€‚
2. **å‰©é¤˜çš„** å¯ä»¥æ˜¯åŸºæ–¼ã€Œç›®æ¨™æ–‡ç« ã€çš„å»¶ä¼¸ ([æœ¬æ–‡å»¶ä¼¸])ã€‚
3. å¦‚æœ RAG æ–‡ç« éå¸¸ç›¸é—œï¼Œæ‚¨å¯ä»¥ç”Ÿæˆè¶…é 3 çµ„ RAG å•ç­” (ä¾‹å¦‚ 4 çµ„ RAG + 2 çµ„ æœ¬æ–‡å»¶ä¼¸)ã€‚
4. çµ•å°ä¸å¯ä»¥å…¨éƒ¨éƒ½æ˜¯ [æœ¬æ–‡å»¶ä¼¸]ã€‚

è«‹ä»¥ JSON é™£åˆ—æ ¼å¼è¼¸å‡ºã€‚
`;

  try {
    const text = await generateWithFallback(async (model) => {
      const response = await ai.models.generateContent({
        model: model,
        contents: prompt,
        config: {
          systemInstruction: systemInstruction, // Dynamic Prompt with injected trends
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.ARRAY,
            items: {
              type: Type.OBJECT,
              properties: {
                question: { type: Type.STRING, description: "å•é¡Œæ¨™é¡Œï¼Œ15å­—ä»¥å…§ï¼Œå¿…é ˆæ˜¯å•å¥å½¢å¼ï¼ˆä¾‹å¦‚ï¼š...æ€éº¼æ­ï¼Ÿ...æ˜¯ä»€éº¼ï¼Ÿï¼‰" },
                answer: { type: Type.STRING, description: "å›ç­”å…§å®¹ï¼Œç´„800-1000å­—ï¼Œéœ€åŒ…å«3å€‹å°æ¨™é¡Œã€‚âš ï¸é‡è¦ï¼šå°æ¨™é¡Œ(**æ–‡å­—**)å¿…é ˆç¨ç«‹ä¸€è¡Œï¼Œå‰å¾Œè«‹æ›è¡Œã€‚åš´æ ¼ç¦æ­¢å‡ºç¾ [ID:xxxxx]ã€‚" },
                sourceId: { type: Type.STRING, description: "åƒè€ƒä¾†æºIDï¼Œä¾‹å¦‚ '57759' æˆ– 'æœ¬æ–‡å»¶ä¼¸'" },
                sourceTitle: { type: Type.STRING, description: "å¿…é ˆå®Œå…¨è¤‡è£½ RAG è³‡æ–™åº«ä¸­çš„ã€åŸå§‹å®Œæ•´æ¨™é¡Œã€‘ï¼Œç¦æ­¢ç°¡åŒ–æˆ–æ”¹å¯«ã€‚" },
              },
              required: ["question", "answer", "sourceId", "sourceTitle"],
            },
          },
        },
      });
      if (!response.text) throw new Error("No response from AI");
      return response.text;
    });

    // Parse output
    const data = JSON.parse(text);
    return data as QAPair[];

  } catch (error) {
    console.error("Gemini API All Models Failed:", error);
    throw error;
  }
};

/**
 * Generates a SINGLE QA pair based on the context.
 * Used for the "Redo" functionality.
 */
export const generateSingleQA = async (request: GenerateRequest): Promise<QAPair> => {
  const { inputArticle, ragContext } = request;

  // 1. Get Daily Trends (Cached or Fetched)
  const todayTrends = await getDailyTrends();

  // 2. Inject Trends into System Prompt
  const systemInstruction = SYSTEM_INSTRUCTION_TEMPLATE.replace('{{today_trends}}', todayTrends);

  const contextString = ragContext.map((ctx, index) =>
    `[æ–‡ç«  ${index + 1}] ID: ${ctx.id}\næ¨™é¡Œ: ${ctx.title}\nå…§å®¹æ‘˜è¦: ${ctx.content}\n`
  ).join('\n----------------\n');

  // Slightly modified prompt to ask for just one high-quality pair
  const prompt = `
**ç›®æ¨™æ–‡ç« å…§å®¹**ï¼š
${inputArticle}

**æª¢ç´¢åˆ°çš„æ­·å²æ–‡ç«  (RAG Context)**ï¼š
${contextString}

è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™ï¼Œæ’°å¯« **1 çµ„** å…¨æ–°çš„å»¶ä¼¸å•ç­” (Q&A)ï¼Œè«‹å˜—è©¦åˆ‡å…¥ä¸åŒçš„è§€é»ã€‚
`;

  try {
    const text = await generateWithFallback(async (model) => {
      const response = await ai.models.generateContent({
        model: model,
        contents: prompt,
        config: {
          systemInstruction: systemInstruction, // Reuse the same persona
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.OBJECT, // Requesting a single Object, not Array
            properties: {
              question: { type: Type.STRING, description: "å•é¡Œæ¨™é¡Œï¼Œ15å­—ä»¥å…§ï¼Œå¿…é ˆæ˜¯å•å¥å½¢å¼ï¼ˆä¾‹å¦‚ï¼š...æ€éº¼æ­ï¼Ÿ...æ˜¯ä»€éº¼ï¼Ÿï¼‰" },
              answer: { type: Type.STRING, description: "å›ç­”å…§å®¹ï¼Œç´„800-1000å­—ï¼Œéœ€åŒ…å«3å€‹å°æ¨™é¡Œã€‚âš ï¸é‡è¦ï¼šå°æ¨™é¡Œ(**æ–‡å­—**)å¿…é ˆç¨ç«‹ä¸€è¡Œï¼Œå‰å¾Œè«‹æ›è¡Œã€‚åš´æ ¼ç¦æ­¢å‡ºç¾ [ID:xxxxx]ã€‚" },
              sourceId: { type: Type.STRING, description: "åƒè€ƒä¾†æºIDï¼Œä¾‹å¦‚ '57759' æˆ– 'æœ¬æ–‡å»¶ä¼¸'" },
              sourceTitle: { type: Type.STRING, description: "å¿…é ˆå®Œå…¨è¤‡è£½ RAG è³‡æ–™åº«ä¸­çš„ã€åŸå§‹å®Œæ•´æ¨™é¡Œã€‘ï¼Œç¦æ­¢ç°¡åŒ–æˆ–æ”¹å¯«ã€‚" },
            },
            required: ["question", "answer", "sourceId", "sourceTitle"],
          },
        },
      });
      if (!response.text) throw new Error("No response from AI");
      return response.text;
    });

    return JSON.parse(text) as QAPair;

  } catch (error) {
    console.error("Gemini API Single Generation Error:", error);
    throw error;
  }
};